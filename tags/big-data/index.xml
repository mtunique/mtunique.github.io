<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>big-data on mtunique blog</title><link>https://mtunique.com/tags/big-data/</link><description>Recent content in big-data on mtunique blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>mtunique</copyright><lastBuildDate>Sun, 26 Mar 2017 22:30:27 +0800</lastBuildDate><atom:link href="https://mtunique.com/tags/big-data/index.xml" rel="self" type="application/rss+xml"/><item><title>Flink Table/SQL API 规划 —— Dynamic Table</title><link>https://mtunique.com/posts/flink_sql_outlook/</link><pubDate>Sun, 26 Mar 2017 22:30:27 +0800</pubDate><guid>https://mtunique.com/posts/flink_sql_outlook/</guid><description>动态表的概念是社区很早就提出的但并没有全部实现下文中所有介绍都是基于已有规划和proposal给出的，可能与之后实现存在出入仅供参考 概念 动态表直观上看是一个类似于数据库中的Materialized View概念。动态表随着时间改变；类似静态的batch table一样可以用标准SQL进行查询然后一个新的动态表；可以和流无损地互相转换(对偶的)。对现有的API最大的改进关键在表的内容随着时间改变，而现在的状态只是append。当前的streaming table可以认为是一种动态表，append模式的动态表。
流到 Dynamic Table 流被转换成Table时决定选择哪种模式是依据表的schema是否定义primary key。
Append模式： 如果表的schema没有包括key的定义那转换成表时采用append模式。把流中每条新来的record当做新的row append到表中。一旦数据加到表中就不能再被更新和删除(指当前表中，不考虑转换成新表)。
Replace模式： 相对应，如果定义了key，那么对于流中的每条记录如果key不在表中就insert否则就update。
Dynamic Table 到 流 表到流的操作是把表的所有change以changelog stream的方式发送到下游。这一步也有两种模式。
Retraction模式： traction模式中对于Dynamic Table的insert和delete的change分别产生insert或delete event。如果是update的change会产生两种change event，对于之前发送出去的同样key的record会产生delete event，对于当前的record是产生insert event。如下图所示：
Update模式： update模式依赖Dynamic Table定义了key。所有的change event是一个kv对。key对应表的key在当前record中的值；对于insert和change value对应新的record。对于delete value是空表示该可以已经被删除。如下图所示：
example 表的内容随着时间改变意味着对表的query结果也是随着时间改变的。我们定义：
A[t]: 时间t时的表A q(A[t])：时间t时对表A执行query q 举个例子来理解动态表的概念：
query的限制 由于流是无限的，相对应 Dynamic Table 也是无界的。当查询无限的表的时候我们需要保证query的定时是良好的，有意义可行的。
1.在实践中Flink将查询转换成持续的流式应用，执行的query仅针对当前的逻辑时间，所以不支持对于任意时间点的查询(A[t])。 2.最直观的原则是query可能的状态和计算必须是有界的，所以可以支持可增量计算的查询：
不断更新当前结果的查询：查询可以产生insert，update和delete更改。查询可以表示为 Q(t+1) = q'(Q(t), c(T, t, t+1))，其中Q(t)是query q的前一次查询结果，c(T, t, t_+1) 是表T从t+1到t的变化, q&amp;rsquo;是q的增量版本。 产生append-only的表，可以从输入表的尾端直接计算出新数据。查询可以表示为 Q(t+1) = q''(c(T, t-x, t+1)) ∪ Q(t)，q'&amp;lsquo;是不需要时间t时q的结果增量版本query q。c(T, t-x, t+1)是表T尾部的x+1个数据，x取决于语义。例如最后一小时的window aggregation至少需要最后一小时的数据作为状态。其他能支持的查询类型还有：单独在每一行上操作的SELECT WHERE；rowtime上的GROUP BY子句（比如基于时间的window aggregate）；ORDER BY rowtime的OVER windows(row-windows)；ORDER BY rowtime。 3.</description></item><item><title>浅析 Flink Table/SQL API</title><link>https://mtunique.com/posts/flink_sql/</link><pubDate>Tue, 21 Mar 2017 22:30:27 +0800</pubDate><guid>https://mtunique.com/posts/flink_sql/</guid><description>从何而来 关系型API有很多好处：是声明式的，用户只需要告诉需要什么，系统决定如何计算；用户不必特地实现；更方便优化，可以执行得更高效。本身Flink就是一个统一批和流的分布式计算平台，所以社区设计关系型API的目的之一是可以让关系型API作为统一的一层，两种查询拥有同样的语义和语法。大多数流处理框架的API都是比较low-level的API，学习成本高而且很多逻辑需要写到UDF中，所以Apache Flink 添加了SQL-like的API处理关系型数据&amp;ndash;Table API。这套API中最重要的概念是Table(可以在上面进行关系型操作的结构化的DataSet或DataStream)。Table API 与 DataSet和DataStream API 结合紧密，DataSet 和 DataStream都可以很容易地转换成 Table，同样转换回来也很方便：
val execEnv = ExecutionEnvironment.getExecutionEnvironment val tableEnv = TableEnvironment.getTableEnvironment(execEnv) // obtain a DataSet from somewhere val tempData: DataSet[(String, Long, Double)] = // convert the DataSet to a Table val tempTable: Table = tempData.toTable(tableEnv, &amp;#39;location, &amp;#39;time, &amp;#39;tempF) // compute your result val avgTempCTable: Table = tempTable .where(&amp;#39;location.like(&amp;#34;room%&amp;#34;)) .select( (&amp;#39;time / (3600 * 24)) as &amp;#39;day, &amp;#39;Location as &amp;#39;room, ((&amp;#39;tempF - 32) * 0.</description></item><item><title>spark和flink的内存管理简单介绍和对比</title><link>https://mtunique.com/posts/spark_flink_mem/</link><pubDate>Sat, 24 Dec 2016 11:30:27 +0800</pubDate><guid>https://mtunique.com/posts/spark_flink_mem/</guid><description>为什么要做内存管理 最近几年存储和网络硬件的升级对大数据领域相关的系统性能提升很大，在很多场景中CPU和内存的渐渐成为瓶颈。 然而大数据领域很多开源框架都使用JVM。相对 c/c++ JVM系语言对于CPU和内存的利用还是差很多的，主要体现在:
GC的开销较大，Full GC更会极大影响性能，尤其是对于为了处理更大数据开辟了极大内存空间的JVM来说。 Java对象的存储密度低。一个对象头就8字节，为了对齐有的时候还要补齐。Java对象的内存很难利用CPU的各级cache，对于 avx simd 等技术很难利用起来难以发挥CPU的这些优势。 OOM的问题影响稳定性。在大数据领域经常会遇到OOM的问题，除了影响稳定性之外，JVM崩溃后重新拉起的代价也很高。 为了解决这些问题spark flink hbase hive 等各大框架都在自己管理内存。框架自己比JVM更了解自己的内存，更熟悉生命周期，拥有更多的信息来管理内存。
// 直接的意思是&amp;quot;绕过&amp;quot;JVM的内存管理机制自己管理每一个字节，就像写C一样，而不是new出来等着GC；另一方面是指通过定制的序列化工具等技术直接操作数据内容不必全部反序列化或不反序列化。
统一管理内存 内存中的Java对象越少, GC压力越小，而且可以保证统一管理的内存一直呆在老年代，而且也可以是堆外内存。
Spark 和 Flink 都支持堆内和堆外两种内存的管理。管理的策略都和操作系统类似，对内存进行分段分页。两者都是一级页表。
Spark：页表的大小是 1 &amp;lt;&amp;lt; 13, 每个页(MemoryBlock)的大小不确定。on-heap模式时候能表示的最大页大小受限于long[]数组的最大长度，理论上最大能表示8192*2^32*****8字节(35T)。on-heap模式时如果页超过1M会触发bufferPools来复用long数组(HeapMemoryAllocator)。TaskMemoryManager接管Task的内存分配释放，&amp;ldquo;寻址&amp;rdquo;，分配的内存页大小不固定，具体执行内存申请是由HeapMemoryAllocator和UnsafeMemoryAllocator进行。
Flink: MemoryManager接管内存分配和释放，构造时页的大小固定，页表大小根据需要的内存反推。TaskManagerServices构造MemoryManager时会将页大小定为networkBufferSize, 默认大小为32KB, 为了配合flink的Network Buffer管理。AbstractPagedInputView, AbstractPagedOutputView以及各种InputView OutputView 用来进行跨page读写内存。
这些缓解了GC耗时和抖动问题。
spill到内存外持久化存储 具体原理和为什么都不叫显然，两者实现也没有什么差别。
Flink: 由内存使用者管理spill规则，并不像操作系统一样。比如: MutableHashTable，SpillingBuffer&amp;hellip; Spark: 同样由各类使用者各自管理，所有内存的消费者都继承MemoryConsumer abstract class， 实现具体spill方法。相对Flink更整洁，统一。BytesToBytesMap，ExternalAppendOnlyMap，ExternalSorter会spill到disk。 列方式存储 根据空间局部性原理，做到缓存友好首先对数据进行向量化存储。通常我们都是对结构化数据的一&amp;quot;列&amp;quot;进行相同的处理，所以
缓存友好算法 codegen (SIMD AVX)</description></item><item><title>如何选择三种Spark API</title><link>https://mtunique.com/posts/spark_api/</link><pubDate>Fri, 11 Nov 2016 14:30:27 +0800</pubDate><guid>https://mtunique.com/posts/spark_api/</guid><description>在这里查看 Apache Spark 2.0 Api 的改进：RDD，DataFrame，DataSet和SQL
Apache Spark 正在快速发展，包括更改和添加核心API。最具破坏性的改变之一是dataset。 Spark 1.0 使用RDD API，但是在最近的十二个月引入了两个新不兼容的API。Spark 1.3 引入了完全不同的DataFrame API 而且最近发布的 Spark 1.6 引入了 Dataset API 浏览版。
很多现有的 Spark 开发者想知道是否应该从 RDDs 直接切换到 Dataset API，或者先切换到 DataFrame API。Spark 新手应该选择哪个 API 开始学习。
这篇文章将提供每个API的概述，并且总结了每个的优缺点。 配套的github repo提供了例子，可以从这开始实验文中提到的方法。yd
RDD (弹性分布式数据集) API 从 1.0 开始一直存在 Spark 中。 这个接口和 Java 版本 JavaRDD 对于已经完成标准Spark教程的任何开发人员来说都是熟悉的。从开发人员的角度来看，RDD只是一组表示数据的Java或Scala对象。
RDD API 提供很多转换方法来在数据上执行计算，比如 map() ， filter() ，和 reduce() 。这些方法的结果表示转换后的新RDD。 然而，这些方法只是定义要执行的操作，直到调用action方法才执行转换。action 方法比如是： collect() 和 saveAsObjectFile().
RDD 转换和action例子 Scala: rdd.</description></item><item><title>怎样利用Spark Streaming和Hadoop实现近实时的会话连接</title><link>https://mtunique.com/posts/spark_streaming_session/</link><pubDate>Tue, 06 Jan 2015 10:20:27 +0800</pubDate><guid>https://mtunique.com/posts/spark_streaming_session/</guid><description>这个 Spark Streaming 样例是一个可持久化到Hadoop近实时会话的很好的例子。
Spark Streaming 是Apache Spark 中最有趣的组件之一。你用Spark Streaming可以创建数据管道来用批量加载数据一样的API处理流式数据。此外，Spark Steaming的“micro-batching”方式提供相当好的弹性来应对一些原因造成的任务失败。
在这篇文章中，我将通过网站的事件近实时回话的例子演示使你熟悉一些常见的和高级的Spark Streaming功能，然后加载活动有关的统计数据到Apache HBase，用不喜欢的BI用具来绘图分析。 (Sessionization指的是捕获的单一访问者的网站会话时间范围内所有点击流活动。)你可以在这里找到了这个演示的代码。
像这样的系统对于了解访问者的行为（无论是人还是机器）是超级有用的。通过一些额外的工作它也可以被设计成windowing模式来以异步方式检测可能的欺诈。
Spark Streaming 代码
我们的例子中的main class是：
com.cloudera.sa.example.sparkstreaming.sessionization.SessionizeData
让我们来看看这段代码段（忽略1-59行，其中包含imports 和其他无聊的东西）。
60到112行：设置Spark Streaming 这些行是非常基本的，用来设置的Spark Streaming，同时可以选择从HDFS或socket接收数据流。如果你在Spark Streaming方面是一个新手，我已经添加了一些详细的注释帮助理解代码。 （我不打算在这里详谈，因为仍然在样例代码里。）
//This is just creating a Spark Config object. I don&amp;#39;t do much here but //add the app name. There are tons of options to put into the Spark config, //but none are needed for this simple example. val sparkConf = new SparkConf().</description></item><item><title>【Spark笔记】基本概念</title><link>https://mtunique.com/posts/spark_base/</link><pubDate>Fri, 28 Nov 2014 13:30:27 +0800</pubDate><guid>https://mtunique.com/posts/spark_base/</guid><description>代码量 Spark: 20,000 LOC
Hadoop 1.0: 90,000 LOC
Hadoop 2.0: 220,000 LOC
基本概念 RDD - resillient distributed dataset 弹性分布式数据集
Operation - 作用于RDD的各种操作分为transformation和action
Job - 作业，一个JOB包含多个RDD及作用于相应RDD上的各种operation
Stage - 一个作业分为多个阶段
Partition - 数据分区， 一个RDD中的数据可以分成多个不同的区
DAG - Directed Acycle graph, 有向无环图，反应RDD之间的依赖关系
Narrow dependency - 窄依赖，子RDD依赖于父RDD中固定的data partition
Wide Dependency - 宽依赖，子RDD对父RDD中的所有data partition都有依赖
Caching Managenment - 缓存管理，对RDD的中间计算结果进行缓存管理以加快整体的处理速度
这些基本概念会反复提到
编程模型 Spark应用程序可分两部分：driver部分和executor部分初始化SparkContext和主体程序
A：driver部分 driver部分主要是对SparkContext进行配置、初始化以及关闭。初始化SparkContext是为了构建Spark应用程序的运行环境，在初始化SparkContext，要先导入一些Spark的类和隐式转换；在executor部分运行完毕后，需要将SparkContext关闭。
B：executor部分 Spark应用程序的executor部分是对数据的处理，数据分三种：
原生数据 包含输入的数据和输出的数据。
输入原生数据 scala集合数据集，如Array(1,2,3,4,5)，Spark使用parallelize方法转换成RDD。 hadoop数据集，Spark支持存储在hadoop上的文件和hadoop支持的其他文件系统，如本地文件、HBase、SequenceFile和Hadoop的输入格式。例如Spark使用txtFile方法可以将本地文件或HDFS文件转换成RDD. 输出数据 生成Scala标量数据，如count（返回RDD中元素的个数）、reduce、fold/aggregate；返回几个标量，如take（返回前几个元素）。 生成Scala集合数据集，如collect（把RDD中的所有元素倒入 Scala集合类型）、lookup（查找对应key的所有值）。 生成hadoop数据集，如saveAsTextFile、saveAsSequenceFile RDD(Resilient Distributed Datasets) 是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。同时，RDD还提供了一组丰富的操作来操作这些数据。在这些操作中，诸如map、flatMap、filter等转换操作实现了monad模式，很好地契合了Scala的集合操作。除此之外，RDD还提供了诸如join、groupBy、reduceByKey等更为方便的操作（注意，reduceByKey是action，而非transformation），以支持常见的数据运算。 RDD作为数据结构，本质上是一个只读的分区记录集合。一个RDD可以包含多个分区，每个分区就是一个dataset片段。RDD可以相互依赖。如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之为narrow dependency；若多个Child RDD分区都可以依赖，则称之为wide dependency。不同的操作依据其特性，可能会产生不同的依赖。例如map操作会产生narrow dependency，而join操作则产生wide dependency。</description></item></channel></rss>