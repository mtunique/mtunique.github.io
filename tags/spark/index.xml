<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>spark on mtunique blog</title><link>https://example.com/tags/spark/</link><description>Recent content in spark on mtunique blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>mtunique</copyright><lastBuildDate>Sat, 24 Dec 2016 11:30:27 +0800</lastBuildDate><atom:link href="https://example.com/tags/spark/index.xml" rel="self" type="application/rss+xml"/><item><title>【未完成】spark和flink的内存管理简单介绍和对比</title><link>https://example.com/posts/spark_flink_mem/</link><pubDate>Sat, 24 Dec 2016 11:30:27 +0800</pubDate><guid>https://example.com/posts/spark_flink_mem/</guid><description>为什么要做内存管理 最近几年存储和网络硬件的升级对大数据领域相关的系统性能提升很大，在很多场景中CPU和内存的渐渐成为瓶颈。 然而大数据领域很多开源框架都使用JVM。相对 c/c++ JVM系语言对于CPU和内存的利用还是差很多的，主要体现在:
GC的开销较大，Full GC更会极大影响性能，尤其是对于为了处理更大数据开辟了极大内存空间的JVM来说。 Java对象的存储密度低。一个对象头就8字节，为了对齐有的时候还要补齐。Java对象的内存很难利用CPU的各级cache，对于 avx simd 等技术很难利用起来难以发挥CPU的这些优势。 OOM的问题影响稳定性。在大数据领域经常会遇到OOM的问题，除了影响稳定性之外，JVM崩溃后重新拉起的代价也很高。 为了解决这些问题spark flink hbase hive 等各大框架都在自己管理内存。框架自己比JVM更了解自己的内存，更熟悉生命周期，拥有更多的信息来管理内存。
// 直接的意思是&amp;quot;绕过&amp;quot;JVM的内存管理机制自己管理每一个字节，就像写C一样，而不是new出来等着GC；另一方面是指通过定制的序列化工具等技术直接操作数据内容不必全部反序列化或不反序列化。
统一管理内存 内存中的Java对象越少, GC压力越小，而且可以保证统一管理的内存一直呆在老年代，而且也可以是堆外内存。
Spark 和 Flink 都支持堆内和堆外两种内存的管理。管理的策略都和操作系统类似，对内存进行分段分页。两者都是一级页表。
Spark：页表的大小是 1 &amp;lt;&amp;lt; 13, 每个页(MemoryBlock)的大小不确定。on-heap模式时候能表示的最大页大小受限于long[]数组的最大长度，理论上最大能表示8192*2^32*****8字节(35T)。on-heap模式时如果页超过1M会触发bufferPools来复用long数组(HeapMemoryAllocator)。TaskMemoryManager接管Task的内存分配释放，&amp;ldquo;寻址&amp;rdquo;，分配的内存页大小不固定，具体执行内存申请是由HeapMemoryAllocator和UnsafeMemoryAllocator进行。
Flink: MemoryManager接管内存分配和释放，构造时页的大小固定，页表大小根据需要的内存反推。TaskManagerServices构造MemoryManager时会将页大小定为networkBufferSize, 默认大小为32KB, 为了配合flink的Network Buffer管理。AbstractPagedInputView, AbstractPagedOutputView以及各种InputView OutputView 用来进行跨page读写内存。
这些缓解了GC耗时和抖动问题。
spill到内存外持久化存储 具体原理和为什么都不叫显然，两者实现也没有什么差别。
Flink: 由内存使用者管理spill规则，并不像操作系统一样。比如: MutableHashTable，SpillingBuffer&amp;hellip; Spark: 同样由各类使用者各自管理，所有内存的消费者都继承MemoryConsumer abstract class， 实现具体spill方法。相对Flink更整洁，统一。BytesToBytesMap，ExternalAppendOnlyMap，ExternalSorter会spill到disk。 列方式存储 根据空间局部性原理，做到缓存友好首先对数据进行向量化存储。通常我们都是对结构化数据的一&amp;quot;列&amp;quot;进行相同的处理，所以
缓存友好算法 codegen (SIMD AVX)</description></item><item><title>如何选择三种Spark API</title><link>https://example.com/posts/spark_api/</link><pubDate>Fri, 11 Nov 2016 14:30:27 +0800</pubDate><guid>https://example.com/posts/spark_api/</guid><description>在这里查看 Apache Spark 2.0 Api 的改进：RDD，DataFrame，DataSet和SQL
Apache Spark 正在快速发展，包括更改和添加核心API。最具破坏性的改变之一是dataset。 Spark 1.0 使用RDD API，但是在最近的十二个月引入了两个新不兼容的API。Spark 1.3 引入了完全不同的DataFrame API 而且最近发布的 Spark 1.6 引入了 Dataset API 浏览版。
很多现有的 Spark 开发者想知道是否应该从 RDDs 直接切换到 Dataset API，或者先切换到 DataFrame API。Spark 新手应该选择哪个 API 开始学习。
这篇文章将提供每个API的概述，并且总结了每个的优缺点。 配套的github repo提供了例子，可以从这开始实验文中提到的方法。yd
RDD (弹性分布式数据集) API 从 1.0 开始一直存在 Spark 中。 这个接口和 Java 版本 JavaRDD 对于已经完成标准Spark教程的任何开发人员来说都是熟悉的。从开发人员的角度来看，RDD只是一组表示数据的Java或Scala对象。
RDD API 提供很多转换方法来在数据上执行计算，比如 map() ， filter() ，和 reduce() 。这些方法的结果表示转换后的新RDD。 然而，这些方法只是定义要执行的操作，直到调用action方法才执行转换。action 方法比如是： collect() 和 saveAsObjectFile().
RDD 转换和action例子 Scala: rdd.</description></item><item><title>【Spark笔记】基本概念</title><link>https://example.com/posts/spark_base/</link><pubDate>Fri, 28 Nov 2014 13:30:27 +0800</pubDate><guid>https://example.com/posts/spark_base/</guid><description>代码量 Spark: 20,000 LOC
Hadoop 1.0: 90,000 LOC
Hadoop 2.0: 220,000 LOC
基本概念 RDD - resillient distributed dataset 弹性分布式数据集
Operation - 作用于RDD的各种操作分为transformation和action
Job - 作业，一个JOB包含多个RDD及作用于相应RDD上的各种operation
Stage - 一个作业分为多个阶段
Partition - 数据分区， 一个RDD中的数据可以分成多个不同的区
DAG - Directed Acycle graph, 有向无环图，反应RDD之间的依赖关系
Narrow dependency - 窄依赖，子RDD依赖于父RDD中固定的data partition
Wide Dependency - 宽依赖，子RDD对父RDD中的所有data partition都有依赖
Caching Managenment - 缓存管理，对RDD的中间计算结果进行缓存管理以加快整体的处理速度
这些基本概念会反复提到
编程模型 Spark应用程序可分两部分：driver部分和executor部分初始化SparkContext和主体程序
A：driver部分 driver部分主要是对SparkContext进行配置、初始化以及关闭。初始化SparkContext是为了构建Spark应用程序的运行环境，在初始化SparkContext，要先导入一些Spark的类和隐式转换；在executor部分运行完毕后，需要将SparkContext关闭。
B：executor部分 Spark应用程序的executor部分是对数据的处理，数据分三种：
原生数据 包含输入的数据和输出的数据。
输入原生数据 scala集合数据集，如Array(1,2,3,4,5)，Spark使用parallelize方法转换成RDD。 hadoop数据集，Spark支持存储在hadoop上的文件和hadoop支持的其他文件系统，如本地文件、HBase、SequenceFile和Hadoop的输入格式。例如Spark使用txtFile方法可以将本地文件或HDFS文件转换成RDD. 输出数据 生成Scala标量数据，如count（返回RDD中元素的个数）、reduce、fold/aggregate；返回几个标量，如take（返回前几个元素）。 生成Scala集合数据集，如collect（把RDD中的所有元素倒入 Scala集合类型）、lookup（查找对应key的所有值）。 生成hadoop数据集，如saveAsTextFile、saveAsSequenceFile RDD(Resilient Distributed Datasets) 是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。同时，RDD还提供了一组丰富的操作来操作这些数据。在这些操作中，诸如map、flatMap、filter等转换操作实现了monad模式，很好地契合了Scala的集合操作。除此之外，RDD还提供了诸如join、groupBy、reduceByKey等更为方便的操作（注意，reduceByKey是action，而非transformation），以支持常见的数据运算。 RDD作为数据结构，本质上是一个只读的分区记录集合。一个RDD可以包含多个分区，每个分区就是一个dataset片段。RDD可以相互依赖。如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之为narrow dependency；若多个Child RDD分区都可以依赖，则称之为wide dependency。不同的操作依据其特性，可能会产生不同的依赖。例如map操作会产生narrow dependency，而join操作则产生wide dependency。</description></item><item><title>怎样利用Spark Streaming和Hadoop实现近实时的会话连接</title><link>https://example.com/posts/spark_streaming_session/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.com/posts/spark_streaming_session/</guid><description>这个 Spark Streaming 样例是一个可持久化到Hadoop近实时会话的很好的例子。
Spark Streaming 是Apache Spark 中最有趣的组件之一。你用Spark Streaming可以创建数据管道来用批量加载数据一样的API处理流式数据。此外，Spark Steaming的“micro-batching”方式提供相当好的弹性来应对一些原因造成的任务失败。
在这篇文章中，我将通过网站的事件近实时回话的例子演示使你熟悉一些常见的和高级的Spark Streaming功能，然后加载活动有关的统计数据到Apache HBase，用不喜欢的BI用具来绘图分析。 (Sessionization指的是捕获的单一访问者的网站会话时间范围内所有点击流活动。)你可以在这里找到了这个演示的代码。
像这样的系统对于了解访问者的行为（无论是人还是机器）是超级有用的。通过一些额外的工作它也可以被设计成windowing模式来以异步方式检测可能的欺诈。
Spark Streaming 代码
我们的例子中的main class是：
com.cloudera.sa.example.sparkstreaming.sessionization.SessionizeData
让我们来看看这段代码段（忽略1-59行，其中包含imports 和其他无聊的东西）。
60到112行：设置Spark Streaming 这些行是非常基本的，用来设置的Spark Streaming，同时可以选择从HDFS或socket接收数据流。如果你在Spark Streaming方面是一个新手，我已经添加了一些详细的注释帮助理解代码。 （我不打算在这里详谈，因为仍然在样例代码里。）
//This is just creating a Spark Config object. I don&amp;#39;t do much here but //add the app name. There are tons of options to put into the Spark config, //but none are needed for this simple example. val sparkConf = new SparkConf().</description></item></channel></rss>