<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on mtunique blog</title><link>https://example.com/posts/</link><description>Recent content in Posts on mtunique blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>mtunique</copyright><lastBuildDate>Sun, 26 Mar 2017 22:30:27 +0800</lastBuildDate><atom:link href="https://example.com/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Flink Table/SQL API 规划 —— Dynamic Table</title><link>https://example.com/posts/flink_sql_outlook/</link><pubDate>Sun, 26 Mar 2017 22:30:27 +0800</pubDate><guid>https://example.com/posts/flink_sql_outlook/</guid><description>动态表的概念是社区很早就提出的但并没有全部实现下文中所有介绍都是基于已有规划和proposal给出的，可能与之后实现存在出入仅供参考 概念 动态表直观上看是一个类似于数据库中的Materialized View概念。动态表随着时间改变；类似静态的batch table一样可以用标准SQL进行查询然后一个新的动态表；可以和流无损地互相转换(对偶的)。对现有的API最大的改进关键在表的内容随着时间改变，而现在的状态只是append。当前的streaming table可以认为是一种动态表，append模式的动态表。
流到 Dynamic Table 流被转换成Table时决定选择哪种模式是依据表的schema是否定义primary key。
Append模式： 如果表的schema没有包括key的定义那转换成表时采用append模式。把流中每条新来的record当做新的row append到表中。一旦数据加到表中就不能再被更新和删除(指当前表中，不考虑转换成新表)。
Replace模式： 相对应，如果定义了key，那么对于流中的每条记录如果key不在表中就insert否则就update。
Dynamic Table 到 流 表到流的操作是把表的所有change以changelog stream的方式发送到下游。这一步也有两种模式。
Retraction模式： traction模式中对于Dynamic Table的insert和delete的change分别产生insert或delete event。如果是update的change会产生两种change event，对于之前发送出去的同样key的record会产生delete event，对于当前的record是产生insert event。如下图所示：
Update模式： update模式依赖Dynamic Table定义了key。所有的change event是一个kv对。key对应表的key在当前record中的值；对于insert和change value对应新的record。对于delete value是空表示该可以已经被删除。如下图所示：
example 表的内容随着时间改变意味着对表的query结果也是随着时间改变的。我们定义：
A[t]: 时间t时的表A q(A[t])：时间t时对表A执行query q 举个例子来理解动态表的概念：
query的限制 由于流是无限的，相对应 Dynamic Table 也是无界的。当查询无限的表的时候我们需要保证query的定时是良好的，有意义可行的。
1.在实践中Flink将查询转换成持续的流式应用，执行的query仅针对当前的逻辑时间，所以不支持对于任意时间点的查询(A[t])。 2.最直观的原则是query可能的状态和计算必须是有界的，所以可以支持可增量计算的查询：
不断更新当前结果的查询：查询可以产生insert，update和delete更改。查询可以表示为 Q(t+1) = q'(Q(t), c(T, t, t+1))，其中Q(t)是query q的前一次查询结果，c(T, t, t_+1) 是表T从t+1到t的变化, q&amp;rsquo;是q的增量版本。 产生append-only的表，可以从输入表的尾端直接计算出新数据。查询可以表示为 Q(t+1) = q''(c(T, t-x, t+1)) ∪ Q(t)，q'&amp;lsquo;是不需要时间t时q的结果增量版本query q。c(T, t-x, t+1)是表T尾部的x+1个数据，x取决于语义。例如最后一小时的window aggregation至少需要最后一小时的数据作为状态。其他能支持的查询类型还有：单独在每一行上操作的SELECT WHERE；rowtime上的GROUP BY子句（比如基于时间的window aggregate）；ORDER BY rowtime的OVER windows(row-windows)；ORDER BY rowtime。 3.</description></item><item><title>浅析 Flink Table/SQL API</title><link>https://example.com/posts/flink_sql/</link><pubDate>Tue, 21 Mar 2017 22:30:27 +0800</pubDate><guid>https://example.com/posts/flink_sql/</guid><description>从何而来 关系型API有很多好处：是声明式的，用户只需要告诉需要什么，系统决定如何计算；用户不必特地实现；更方便优化，可以执行得更高效。本身Flink就是一个统一批和流的分布式计算平台，所以社区设计关系型API的目的之一是可以让关系型API作为统一的一层，两种查询拥有同样的语义和语法。大多数流处理框架的API都是比较low-level的API，学习成本高而且很多逻辑需要写到UDF中，所以Apache Flink 添加了SQL-like的API处理关系型数据&amp;ndash;Table API。这套API中最重要的概念是Table(可以在上面进行关系型操作的结构化的DataSet或DataStream)。Table API 与 DataSet和DataStream API 结合紧密，DataSet 和 DataStream都可以很容易地转换成 Table，同样转换回来也很方便：
val execEnv = ExecutionEnvironment.getExecutionEnvironment val tableEnv = TableEnvironment.getTableEnvironment(execEnv) // obtain a DataSet from somewhere val tempData: DataSet[(String, Long, Double)] = // convert the DataSet to a Table val tempTable: Table = tempData.toTable(tableEnv, &amp;#39;location, &amp;#39;time, &amp;#39;tempF) // compute your result val avgTempCTable: Table = tempTable .where(&amp;#39;location.like(&amp;#34;room%&amp;#34;)) .select( (&amp;#39;time / (3600 * 24)) as &amp;#39;day, &amp;#39;Location as &amp;#39;room, ((&amp;#39;tempF - 32) * 0.</description></item><item><title>【未完成】spark和flink的内存管理简单介绍和对比</title><link>https://example.com/posts/spark_flink_mem/</link><pubDate>Sat, 24 Dec 2016 11:30:27 +0800</pubDate><guid>https://example.com/posts/spark_flink_mem/</guid><description>为什么要做内存管理 最近几年存储和网络硬件的升级对大数据领域相关的系统性能提升很大，在很多场景中CPU和内存的渐渐成为瓶颈。 然而大数据领域很多开源框架都使用JVM。相对 c/c++ JVM系语言对于CPU和内存的利用还是差很多的，主要体现在:
GC的开销较大，Full GC更会极大影响性能，尤其是对于为了处理更大数据开辟了极大内存空间的JVM来说。 Java对象的存储密度低。一个对象头就8字节，为了对齐有的时候还要补齐。Java对象的内存很难利用CPU的各级cache，对于 avx simd 等技术很难利用起来难以发挥CPU的这些优势。 OOM的问题影响稳定性。在大数据领域经常会遇到OOM的问题，除了影响稳定性之外，JVM崩溃后重新拉起的代价也很高。 为了解决这些问题spark flink hbase hive 等各大框架都在自己管理内存。框架自己比JVM更了解自己的内存，更熟悉生命周期，拥有更多的信息来管理内存。
// 直接的意思是&amp;quot;绕过&amp;quot;JVM的内存管理机制自己管理每一个字节，就像写C一样，而不是new出来等着GC；另一方面是指通过定制的序列化工具等技术直接操作数据内容不必全部反序列化或不反序列化。
统一管理内存 内存中的Java对象越少, GC压力越小，而且可以保证统一管理的内存一直呆在老年代，而且也可以是堆外内存。
Spark 和 Flink 都支持堆内和堆外两种内存的管理。管理的策略都和操作系统类似，对内存进行分段分页。两者都是一级页表。
Spark：页表的大小是 1 &amp;lt;&amp;lt; 13, 每个页(MemoryBlock)的大小不确定。on-heap模式时候能表示的最大页大小受限于long[]数组的最大长度，理论上最大能表示8192*2^32*****8字节(35T)。on-heap模式时如果页超过1M会触发bufferPools来复用long数组(HeapMemoryAllocator)。TaskMemoryManager接管Task的内存分配释放，&amp;ldquo;寻址&amp;rdquo;，分配的内存页大小不固定，具体执行内存申请是由HeapMemoryAllocator和UnsafeMemoryAllocator进行。
Flink: MemoryManager接管内存分配和释放，构造时页的大小固定，页表大小根据需要的内存反推。TaskManagerServices构造MemoryManager时会将页大小定为networkBufferSize, 默认大小为32KB, 为了配合flink的Network Buffer管理。AbstractPagedInputView, AbstractPagedOutputView以及各种InputView OutputView 用来进行跨page读写内存。
这些缓解了GC耗时和抖动问题。
spill到内存外持久化存储 具体原理和为什么都不叫显然，两者实现也没有什么差别。
Flink: 由内存使用者管理spill规则，并不像操作系统一样。比如: MutableHashTable，SpillingBuffer&amp;hellip; Spark: 同样由各类使用者各自管理，所有内存的消费者都继承MemoryConsumer abstract class， 实现具体spill方法。相对Flink更整洁，统一。BytesToBytesMap，ExternalAppendOnlyMap，ExternalSorter会spill到disk。 列方式存储 根据空间局部性原理，做到缓存友好首先对数据进行向量化存储。通常我们都是对结构化数据的一&amp;quot;列&amp;quot;进行相同的处理，所以
缓存友好算法 codegen (SIMD AVX)</description></item><item><title>Docker 调试技巧</title><link>https://example.com/posts/docker_debug/</link><pubDate>Sat, 17 Dec 2016 22:30:27 +0800</pubDate><guid>https://example.com/posts/docker_debug/</guid><description>『重用』容器名 但我们在编写/调试Dockerfile的时候我们经常会重复之前的command，比如这种docker run --name jstorm-zookeeper zookeeper:3.4，然后就容器名就冲突了。
$ docker run --name jstorm-zookeeper zookeeper:3.4 ... $ docker run --name jstorm-zookeeper zookeeper:3.4 docker: Error response from daemon: Conflict. The name &amp;#34;/jstorm-zookeeper&amp;#34; is already in use by container xxxxxxxxx 可以在运行 docker run 时候加上--rm flag, 容器将在退出之后销毁。无需手动docker rm CONTAINER
$ docker run --name jstorm-zookeeper zookeeper:3.4 --rm # reuse $ docker create --name jstorm-zookeeper zookeeper:3.4 $ docker start jstorm-zookeeper # no error debug Dockerfile 在写 Dockerfile 的时候，通常并不会一气呵成。有的时候容器启动就crash 直接退出，有的时候build image 就会失败，或者想验证Dockerfile是否符合预期，我们经常要debug Dockerfile。</description></item><item><title>如何选择三种Spark API</title><link>https://example.com/posts/spark_api/</link><pubDate>Fri, 11 Nov 2016 14:30:27 +0800</pubDate><guid>https://example.com/posts/spark_api/</guid><description>在这里查看 Apache Spark 2.0 Api 的改进：RDD，DataFrame，DataSet和SQL
Apache Spark 正在快速发展，包括更改和添加核心API。最具破坏性的改变之一是dataset。 Spark 1.0 使用RDD API，但是在最近的十二个月引入了两个新不兼容的API。Spark 1.3 引入了完全不同的DataFrame API 而且最近发布的 Spark 1.6 引入了 Dataset API 浏览版。
很多现有的 Spark 开发者想知道是否应该从 RDDs 直接切换到 Dataset API，或者先切换到 DataFrame API。Spark 新手应该选择哪个 API 开始学习。
这篇文章将提供每个API的概述，并且总结了每个的优缺点。 配套的github repo提供了例子，可以从这开始实验文中提到的方法。yd
RDD (弹性分布式数据集) API 从 1.0 开始一直存在 Spark 中。 这个接口和 Java 版本 JavaRDD 对于已经完成标准Spark教程的任何开发人员来说都是熟悉的。从开发人员的角度来看，RDD只是一组表示数据的Java或Scala对象。
RDD API 提供很多转换方法来在数据上执行计算，比如 map() ， filter() ，和 reduce() 。这些方法的结果表示转换后的新RDD。 然而，这些方法只是定义要执行的操作，直到调用action方法才执行转换。action 方法比如是： collect() 和 saveAsObjectFile().
RDD 转换和action例子 Scala: rdd.</description></item><item><title>另一个Lambda表达式教程</title><link>https://example.com/posts/lambda_python/</link><pubDate>Sat, 24 Jan 2015 14:30:00 +0800</pubDate><guid>https://example.com/posts/lambda_python/</guid><description>有很多Python的lambda教程[1]。最近我偶然发现一个，真挺有用的。是Mike Driscoll在Mouse VS Python 博客上的关于lambda的讨论) 。
当我刚开始学习Python，最容易困惑的概念之一，是lambda语句。我敢肯定， 其他新的程序员也对它很困惑... Mike的讨论非常好：清晰，直接，且含有实用的示例。它帮助我终于领会了lambda，并导致我写的另一篇lambda教程。
一个用来构造函数的工具 基本上，Python的lambda是用于构造函数（或更精确地说，函数对象）的工具。这意味着，Python有两个构造函数的工具：def和lambda。
下面是一个例子。您可以以正常的方式用def构造一个函数，就像这样：
def square_root(x): return math.sqrt(x) 或者你可以用lambda
square_root = lambda x: math.sqrt(x) 下面是lambda的其他的一些有趣的例子：
sum = lambda x, y: x + y # def sum(x,y): return x + y out = lambda *x: sys.stdout.write(&amp;#34; &amp;#34;.join(map(str,x))) lambda event, name=button8.getLabel(): self.onButton(event, name) lambda的好处在哪里？ 已经困扰我有很长一段时间的一个问题是：lambda的好处在哪里？为什么我们需要lambda？
答案是： 我们并不需要lambda，我们不用它一样可以做所有的事情。但是… 在特定的情况下，很是方便 - 它让编写代码更容易一些，而且编写的代码更整洁。
什么样的情况？ 好，其中一个情况是，我们需要一个简单的一次性功能：将被只使用一次函数。
通常，写函数有两个目的：(a)以减少代码重复（b）模块化代码。
如果你的应用程序在不同的地方包含重复的代码块，那么你就可以把代码拷贝到一个函数，给函数名，然后 – 使用该函数名 - 在代码中的不同位置调用它。 如果你有一个代码块执行一个明确的操作 - 但真的是冗长、粗糙、破坏程序的可读性，那么你可以把那么长的粗糙的所有代码变成一个函数。 但是，假设你需要创建一个函数，将只被使用一次 - 只从应用程序中的一个地方调用。好吧，首先，你不需要给函数的名称。它可以是“匿名的”。而且你可以把它定义在你想使用它的地方。这就是lambda是非常有用的时候。</description></item><item><title>Linux下用户空间调试工具</title><link>https://example.com/posts/linux_userspace/</link><pubDate>Mon, 29 Dec 2014 17:51:27 +0800</pubDate><guid>https://example.com/posts/linux_userspace/</guid><description>根据定义，调试工具是那些那些使我们能够监测、控制和纠正其他程序的程序。我们为什么应该用调试工具呢? 在有些情况下，运行一些程序的时候我们会被卡住，我们需要明白究竟发生了什么。 例如, 我们正在运行应用程序，它产生了一些错误消息。要修复这些错误，我们应该先找出为什么产生这些错误的消息和这些错误消息从哪里产生的。 一个应用程序可能突然挂起，我们必须了解其他什么进程同时在运行。我们可能还必须弄清楚进程x挂起的时候在做什么。为了剖析这些细节， 我们需要调试工具的帮助。有几个Linux下的用户空间调试工具和技术，他们用来分析用户空间问题相当有用。他们是:
print语句 查询 (/proc, /sys etc) 跟踪 (strace/ltrace) Valgrind (memwatch) GDB 让我们一个个地了解。
1.print语句 这是一个基本的原始的调试问题的方法。 我们可以在程序中插入print语句来了解控制流和变量值。 虽然这是一个简单的技术, 但它有一些缺点的。 程序需要进行编辑以添加print语句，然后不得不重新编译，重新运行来获得输出。 如果要调试的程序相当大，这是一个耗时的方法。
2. 查询 在某些情况下，我们需要弄清楚在一个运行在内核中的进程的状态和内存映射。为了获得这些信息，我们不需要在内核中插入任何代码。 相反，可以用 /proc 文件系统。
/proc 是一个伪文件系统，系统一起启动运行就收集着运行时系统的信息 (cpu信息, 内存容量 等)。
ls /proc的输出
正如你看到的, 系统中运行的每一个进程在/proc文件系统中有一个以进程id命名的项。每个进程的细节信息可以在进程id对应的目录下的文件中获得。
ls /proc/pid的输出
解释/proc文件系统内的所有条目超出了本文的范围。一些有用的列举如下：
/proc/cmdline -&amp;gt; 内核命令行 /proc/cpuinfo -&amp;gt; 关于处理器的品牌，型号信息等 /proc/filesystems -&amp;gt; 文件系统的内核支持的信息 /proc//cmdline -&amp;gt; 命令行参数传递到当前进程 /proc//mem -&amp;gt; 当前进程持有的内存 /proc//status -&amp;gt; 当前进程的状态 3. 跟踪 strace的和ltrace是两个在Linux中用来追踪程序的执行细节的跟踪工具
strace: strace拦截和记录系统调用并且由它来接收的信号。对于用户，它显示了系统调用，传递给它们的参数和返回值。 strace的可以附着到已在运行的进程中，或到一个新的进程。它作为一个针对开发者和系统管理员的诊断，调试工具是很有用的。它也可以用来当为一个通过跟踪不同的程序调用来了解系统的工具。这个工具的好处是不需要源代码和程序不需要重新编译。</description></item><item><title>【Spark笔记】基本概念</title><link>https://example.com/posts/spark_base/</link><pubDate>Fri, 28 Nov 2014 13:30:27 +0800</pubDate><guid>https://example.com/posts/spark_base/</guid><description>代码量 Spark: 20,000 LOC
Hadoop 1.0: 90,000 LOC
Hadoop 2.0: 220,000 LOC
基本概念 RDD - resillient distributed dataset 弹性分布式数据集
Operation - 作用于RDD的各种操作分为transformation和action
Job - 作业，一个JOB包含多个RDD及作用于相应RDD上的各种operation
Stage - 一个作业分为多个阶段
Partition - 数据分区， 一个RDD中的数据可以分成多个不同的区
DAG - Directed Acycle graph, 有向无环图，反应RDD之间的依赖关系
Narrow dependency - 窄依赖，子RDD依赖于父RDD中固定的data partition
Wide Dependency - 宽依赖，子RDD对父RDD中的所有data partition都有依赖
Caching Managenment - 缓存管理，对RDD的中间计算结果进行缓存管理以加快整体的处理速度
这些基本概念会反复提到
编程模型 Spark应用程序可分两部分：driver部分和executor部分初始化SparkContext和主体程序
A：driver部分 driver部分主要是对SparkContext进行配置、初始化以及关闭。初始化SparkContext是为了构建Spark应用程序的运行环境，在初始化SparkContext，要先导入一些Spark的类和隐式转换；在executor部分运行完毕后，需要将SparkContext关闭。
B：executor部分 Spark应用程序的executor部分是对数据的处理，数据分三种：
原生数据 包含输入的数据和输出的数据。
输入原生数据 scala集合数据集，如Array(1,2,3,4,5)，Spark使用parallelize方法转换成RDD。 hadoop数据集，Spark支持存储在hadoop上的文件和hadoop支持的其他文件系统，如本地文件、HBase、SequenceFile和Hadoop的输入格式。例如Spark使用txtFile方法可以将本地文件或HDFS文件转换成RDD. 输出数据 生成Scala标量数据，如count（返回RDD中元素的个数）、reduce、fold/aggregate；返回几个标量，如take（返回前几个元素）。 生成Scala集合数据集，如collect（把RDD中的所有元素倒入 Scala集合类型）、lookup（查找对应key的所有值）。 生成hadoop数据集，如saveAsTextFile、saveAsSequenceFile RDD(Resilient Distributed Datasets) 是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。同时，RDD还提供了一组丰富的操作来操作这些数据。在这些操作中，诸如map、flatMap、filter等转换操作实现了monad模式，很好地契合了Scala的集合操作。除此之外，RDD还提供了诸如join、groupBy、reduceByKey等更为方便的操作（注意，reduceByKey是action，而非transformation），以支持常见的数据运算。 RDD作为数据结构，本质上是一个只读的分区记录集合。一个RDD可以包含多个分区，每个分区就是一个dataset片段。RDD可以相互依赖。如果RDD的每个分区最多只能被一个Child RDD的一个分区使用，则称之为narrow dependency；若多个Child RDD分区都可以依赖，则称之为wide dependency。不同的操作依据其特性，可能会产生不同的依赖。例如map操作会产生narrow dependency，而join操作则产生wide dependency。</description></item><item><title>【Tornado笔记】tornado.web.Application</title><link>https://example.com/posts/tornado_application/</link><pubDate>Fri, 28 Nov 2014 13:30:27 +0800</pubDate><guid>https://example.com/posts/tornado_application/</guid><description>从tornado的 Hello,world 开始分析tornado的源码
import tornado.ioloop import tornado.web class MainHandler(tornado.web.RequestHandler): def get(self): self.write(&amp;#34;Hello, world&amp;#34;) application = tornado.web.Application([ (r&amp;#34;/&amp;#34;, MainHandler), ]) if __name__ == &amp;#34;__main__&amp;#34;: application.listen(8888) tornado.ioloop.IOLoop.instance().start() 很容易可以看出，通过继承RequestHandler类定义自己的处理类，来处理请求。Application类的对象来处理URI的路由（将URIr&amp;quot;/&amp;quot;于处理类MainHandler组成tuple，关联起来）。
tornado.web.Application类 一、init 简化版代码：
def __init__(self, handlers=None, default_host=&amp;#34;&amp;#34;, transforms=None, **settings): if transforms is None: self.transforms = [] if settings.get(&amp;#34;compress_response&amp;#34;) or settings.get(&amp;#34;gzip&amp;#34;): self.transforms.append(GZipContentEncoding) else: self.transforms = transforms ...... self.ui_modules = {&amp;#39;linkify&amp;#39;: _linkify, &amp;#39;xsrf_form_html&amp;#39;: _xsrf_form_html, &amp;#39;Template&amp;#39;: TemplateModule, } self.ui_methods = {} self._load_ui_modules(settings.get(&amp;#34;ui_modules&amp;#34;, {})) self._load_ui_methods(settings.get(&amp;#34;ui_methods&amp;#34;, {})) if self.settings.get(&amp;#34;static_path&amp;#34;): .</description></item><item><title>【Tornado笔记】tornado.web.IOLoop和tornado.util.Configurable</title><link>https://example.com/posts/tornado_ioloop/</link><pubDate>Fri, 28 Nov 2014 13:30:27 +0800</pubDate><guid>https://example.com/posts/tornado_ioloop/</guid><description>上一篇文章tornado的 Hello, world 还没有分析完 还差最后一句
import tornado.ioloop import tornado.web class MainHandler(tornado.web.RequestHandler): def get(self): self.write(&amp;#34;Hello, world&amp;#34;) application = tornado.web.Application([ (r&amp;#34;/&amp;#34;, MainHandler), ]) if __name__ == &amp;#34;__main__&amp;#34;: application.listen(8888) tornado.ioloop.IOLoop.instance().start() IOLoop就是整个tornado区别于其他框架的最关键的地方。
一、tornado.ioloop.IOLoop.instance @staticmethod def instance(): if not hasattr(IOLoop, &amp;#34;_instance&amp;#34;): with IOLoop._instance_lock: if not hasattr(IOLoop, &amp;#34;_instance&amp;#34;): # New instance after double check IOLoop._instance = IOLoop() return IOLoop._instance 每个tornado进程都会有一个全局的IOLoop实例，这个方法就是用来获得这个实例的。
with IOLoop._instance_lock保证了创建的这个实例的过程是线程安全的。
IOLoop继承了tornado.util.Configurable。Configurable （是一个“抽象”类）实现了__new__，所以将自动调用了__new__生成IOLoop的实例。
二、tornado.util.Configurable.__new__() def __new__(cls, **kwargs): base = cls.configurable_base() args = {} if cls is base: impl = cls.</description></item><item><title>python实现各基本数据结构（1）——离散和线性</title><link>https://example.com/posts/python_datastruct/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.com/posts/python_datastruct/</guid><description>离散 集合（set） Operation Average case Worst Case notes x in s O(1) O(n) Union s t O(len(s)+len(t)) Intersection s&amp;amp;t O(min(len(s), len(t)) O(len(s) * len(t)) replace &amp;ldquo;min&amp;rdquo; with &amp;ldquo;max&amp;rdquo; if t is not a set Multiple intersection s1&amp;amp;s2&amp;amp;..&amp;amp;sn (n-1)*O(l) where l is max(len(s1),..,len(sn)) Difference s-t O(len(s)) s.difference_update(t) O(len(t)) Symmetric Difference s^t O(len(s)) O(len(s) * len(t)) s.</description></item><item><title>怎样利用Spark Streaming和Hadoop实现近实时的会话连接</title><link>https://example.com/posts/spark_streaming_session/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://example.com/posts/spark_streaming_session/</guid><description>这个 Spark Streaming 样例是一个可持久化到Hadoop近实时会话的很好的例子。
Spark Streaming 是Apache Spark 中最有趣的组件之一。你用Spark Streaming可以创建数据管道来用批量加载数据一样的API处理流式数据。此外，Spark Steaming的“micro-batching”方式提供相当好的弹性来应对一些原因造成的任务失败。
在这篇文章中，我将通过网站的事件近实时回话的例子演示使你熟悉一些常见的和高级的Spark Streaming功能，然后加载活动有关的统计数据到Apache HBase，用不喜欢的BI用具来绘图分析。 (Sessionization指的是捕获的单一访问者的网站会话时间范围内所有点击流活动。)你可以在这里找到了这个演示的代码。
像这样的系统对于了解访问者的行为（无论是人还是机器）是超级有用的。通过一些额外的工作它也可以被设计成windowing模式来以异步方式检测可能的欺诈。
Spark Streaming 代码
我们的例子中的main class是：
com.cloudera.sa.example.sparkstreaming.sessionization.SessionizeData
让我们来看看这段代码段（忽略1-59行，其中包含imports 和其他无聊的东西）。
60到112行：设置Spark Streaming 这些行是非常基本的，用来设置的Spark Streaming，同时可以选择从HDFS或socket接收数据流。如果你在Spark Streaming方面是一个新手，我已经添加了一些详细的注释帮助理解代码。 （我不打算在这里详谈，因为仍然在样例代码里。）
//This is just creating a Spark Config object. I don&amp;#39;t do much here but //add the app name. There are tons of options to put into the Spark config, //but none are needed for this simple example. val sparkConf = new SparkConf().</description></item></channel></rss>