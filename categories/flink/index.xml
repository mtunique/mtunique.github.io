<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>flink on mtunique blog</title><link>https://example.com/categories/flink/</link><description>Recent content in flink on mtunique blog</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><copyright>mtunique</copyright><lastBuildDate>Sun, 26 Mar 2017 22:30:27 +0800</lastBuildDate><atom:link href="https://example.com/categories/flink/index.xml" rel="self" type="application/rss+xml"/><item><title>Flink Table/SQL API 规划 —— Dynamic Table</title><link>https://example.com/posts/flink_sql_outlook/</link><pubDate>Sun, 26 Mar 2017 22:30:27 +0800</pubDate><guid>https://example.com/posts/flink_sql_outlook/</guid><description>动态表的概念是社区很早就提出的但并没有全部实现下文中所有介绍都是基于已有规划和proposal给出的，可能与之后实现存在出入仅供参考 概念 动态表直观上看是一个类似于数据库中的Materialized View概念。动态表随着时间改变；类似静态的batch table一样可以用标准SQL进行查询然后一个新的动态表；可以和流无损地互相转换(对偶的)。对现有的API最大的改进关键在表的内容随着时间改变，而现在的状态只是append。当前的streaming table可以认为是一种动态表，append模式的动态表。
流到 Dynamic Table 流被转换成Table时决定选择哪种模式是依据表的schema是否定义primary key。
Append模式： 如果表的schema没有包括key的定义那转换成表时采用append模式。把流中每条新来的record当做新的row append到表中。一旦数据加到表中就不能再被更新和删除(指当前表中，不考虑转换成新表)。
Replace模式： 相对应，如果定义了key，那么对于流中的每条记录如果key不在表中就insert否则就update。
Dynamic Table 到 流 表到流的操作是把表的所有change以changelog stream的方式发送到下游。这一步也有两种模式。
Retraction模式： traction模式中对于Dynamic Table的insert和delete的change分别产生insert或delete event。如果是update的change会产生两种change event，对于之前发送出去的同样key的record会产生delete event，对于当前的record是产生insert event。如下图所示：
Update模式： update模式依赖Dynamic Table定义了key。所有的change event是一个kv对。key对应表的key在当前record中的值；对于insert和change value对应新的record。对于delete value是空表示该可以已经被删除。如下图所示：
example 表的内容随着时间改变意味着对表的query结果也是随着时间改变的。我们定义：
A[t]: 时间t时的表A q(A[t])：时间t时对表A执行query q 举个例子来理解动态表的概念：
query的限制 由于流是无限的，相对应 Dynamic Table 也是无界的。当查询无限的表的时候我们需要保证query的定时是良好的，有意义可行的。
1.在实践中Flink将查询转换成持续的流式应用，执行的query仅针对当前的逻辑时间，所以不支持对于任意时间点的查询(A[t])。 2.最直观的原则是query可能的状态和计算必须是有界的，所以可以支持可增量计算的查询：
不断更新当前结果的查询：查询可以产生insert，update和delete更改。查询可以表示为 Q(t+1) = q'(Q(t), c(T, t, t+1))，其中Q(t)是query q的前一次查询结果，c(T, t, t_+1) 是表T从t+1到t的变化, q&amp;rsquo;是q的增量版本。 产生append-only的表，可以从输入表的尾端直接计算出新数据。查询可以表示为 Q(t+1) = q''(c(T, t-x, t+1)) ∪ Q(t)，q'&amp;lsquo;是不需要时间t时q的结果增量版本query q。c(T, t-x, t+1)是表T尾部的x+1个数据，x取决于语义。例如最后一小时的window aggregation至少需要最后一小时的数据作为状态。其他能支持的查询类型还有：单独在每一行上操作的SELECT WHERE；rowtime上的GROUP BY子句（比如基于时间的window aggregate）；ORDER BY rowtime的OVER windows(row-windows)；ORDER BY rowtime。 3.</description></item><item><title>浅析 Flink Table/SQL API</title><link>https://example.com/posts/flink_sql/</link><pubDate>Tue, 21 Mar 2017 22:30:27 +0800</pubDate><guid>https://example.com/posts/flink_sql/</guid><description>从何而来 关系型API有很多好处：是声明式的，用户只需要告诉需要什么，系统决定如何计算；用户不必特地实现；更方便优化，可以执行得更高效。本身Flink就是一个统一批和流的分布式计算平台，所以社区设计关系型API的目的之一是可以让关系型API作为统一的一层，两种查询拥有同样的语义和语法。大多数流处理框架的API都是比较low-level的API，学习成本高而且很多逻辑需要写到UDF中，所以Apache Flink 添加了SQL-like的API处理关系型数据&amp;ndash;Table API。这套API中最重要的概念是Table(可以在上面进行关系型操作的结构化的DataSet或DataStream)。Table API 与 DataSet和DataStream API 结合紧密，DataSet 和 DataStream都可以很容易地转换成 Table，同样转换回来也很方便：
val execEnv = ExecutionEnvironment.getExecutionEnvironment val tableEnv = TableEnvironment.getTableEnvironment(execEnv) // obtain a DataSet from somewhere val tempData: DataSet[(String, Long, Double)] = // convert the DataSet to a Table val tempTable: Table = tempData.toTable(tableEnv, &amp;#39;location, &amp;#39;time, &amp;#39;tempF) // compute your result val avgTempCTable: Table = tempTable .where(&amp;#39;location.like(&amp;#34;room%&amp;#34;)) .select( (&amp;#39;time / (3600 * 24)) as &amp;#39;day, &amp;#39;Location as &amp;#39;room, ((&amp;#39;tempF - 32) * 0.</description></item></channel></rss>